{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The No U-Turn Sampler\n",
    "\n",
    "Hoffman & Gelman 2011: The No U-Turn Sampler\n",
    "* implement algorithms 1 (basic HMC) & 6 (NUTS with dual averaging)\n",
    "* test that your implementations work by replicating the following tests in Section 4.1:\n",
    " * 250-dimensional Gaussian\n",
    " * Bayesian logistic regression & hierarchical Bayesian logistic regression on the UCI german credit data\n",
    " * compare the results given by basic HMC & NUTS\n",
    " * _compare your results with the same models run in Stan using the provided NUTS sampler_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare your results with the same models run in Stan using the provided NUTS sampler\n",
    "## Download  German credit dataset\n",
    "There are 24 predictors (x). Predictors are normalized to have a zero mean and an unit variance.\n",
    "\n",
    "Outcome is whether a person should be refused to get a credit or approced to get one. Values for the outcome are -1 and 1, after recoding 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "# Download data and standardize X\n",
    "# Should have values 1 or -1\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import pystan\n",
    "%matplotlib inline\n",
    "import HMC\n",
    "import noUturnSampler\n",
    "import ChainMix\n",
    "import Neff\n",
    "import LikelihoodFunctions\n",
    "#import CovergenceTests\n",
    "import importlib\n",
    "importlib.reload(HMC)\n",
    "importlib.reload(ChainMix)\n",
    "importlib.reload(noUturnSampler)\n",
    "importlib.reload(Neff)\n",
    "importlib.reload(LikelihoodFunctions)\n",
    "\n",
    "npr.seed(42)\n",
    "np.seterr('warn')\n",
    "\n",
    "credit0= pd.read_csv('dataGerman.tab', delim_whitespace=True)\n",
    "\n",
    "creditY=np.array(credit0['CREDITRATING'])\n",
    "creditY[creditY==-1]=0\n",
    "\n",
    "creditX0=np.array(credit0.loc[:, credit0.columns != 'CREDITRATING'])\n",
    "creditX=(creditX0 - np.mean(creditX0, axis=0))/np.std(creditX0, axis=0)\n",
    "\n",
    "# Solve beta values for comparison\n",
    "logistic = LogisticRegression(fit_intercept=False)\n",
    "betaCoefficients=logistic.fit(creditX,creditY).coef_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian logistic regression using German credit data\n",
    "Target distribution is the posterior of a Bayesian logistic regression model fit to the German credit dataset.\n",
    "\n",
    "There are 24-beta coefficients associated with the 24 predictors, and intercept alpha. Coefficients and the intercept are given normal priors with variance of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_3e737708637464be719ee19b1a2c8195 NOW.\n"
     ]
    }
   ],
   "source": [
    "# Bayesian logistic regression\n",
    "blrModel = \"\"\"\n",
    "    data {\n",
    "      int<lower=0> N;\n",
    "      int<lower=0> K;   // number of predictors      \n",
    "      matrix[N,K] x;\n",
    "      int<lower=0,upper=1> y[N];\n",
    "    }\n",
    "    parameters {\n",
    "      real alpha;\n",
    "      vector[K] beta;\n",
    "    }\n",
    "    model {\n",
    "      alpha~normal(0,100);\n",
    "      beta~normal(0,100);  \n",
    "      y ~ bernoulli_logit(alpha + x*beta);\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "credit_dat = {'x': creditX,\n",
    "                'y': creditY,\n",
    "                'N': len(creditY),\n",
    "                'K': creditX.shape[1]\n",
    "             }\n",
    "\n",
    "smBlr = pystan.StanModel(model_code=blrModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitBlrNuts = smBlr.sampling(data=credit_dat, iter=1000, chains=4, algorithm='NUTS', control=dict(max_treedepth=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference for Stan model: anon_model_3e737708637464be719ee19b1a2c8195.\n",
       "4 chains, each with iter=1000; warmup=500; thin=1; \n",
       "post-warmup draws per chain=500, total post-warmup draws=2000.\n",
       "\n",
       "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
       "alpha     -1.22  1.8e-3   0.09   -1.4  -1.28  -1.22  -1.16  -1.05   2734    1.0\n",
       "beta[1]   -0.75  1.7e-3   0.09  -0.94  -0.81  -0.75  -0.69  -0.59   2805    1.0\n",
       "beta[2]    0.44  2.0e-3    0.1   0.23   0.37   0.44   0.51   0.65   2806    1.0\n",
       "beta[3]   -0.41  2.0e-3   0.11  -0.62  -0.48  -0.41  -0.34   -0.2   2802    1.0\n",
       "beta[4]    0.16  2.1e-3   0.11  -0.06   0.09   0.16   0.23   0.37   2653    1.0\n",
       "beta[5]   -0.38  1.8e-3    0.1  -0.58  -0.45  -0.38  -0.32   -0.2   2901    1.0\n",
       "beta[6]   -0.19  1.7e-3   0.09  -0.37  -0.25  -0.19  -0.13-7.8e-3   2919    1.0\n",
       "beta[7]   -0.16  1.7e-3   0.08  -0.32  -0.21  -0.15   -0.1   0.01   2432    1.0\n",
       "beta[8]    0.01  1.6e-3   0.09  -0.17  -0.05 8.7e-3   0.07    0.2   3440    1.0\n",
       "beta[9]    0.19  2.3e-3   0.11  -0.03   0.12   0.19   0.26    0.4   2216    1.0\n",
       "beta[10]  -0.11  1.8e-3    0.1  -0.31  -0.18  -0.11  -0.04   0.09   2895    1.0\n",
       "beta[11]  -0.24  1.5e-3   0.08   -0.4   -0.3  -0.24  -0.19  -0.08   3144    1.0\n",
       "beta[12]   0.19  2.1e-3    0.1  -0.02   0.11   0.18   0.26   0.39   2492    1.0\n",
       "beta[13]   0.03  1.6e-3   0.09  -0.15  -0.03   0.03   0.09   0.19   3133    1.0\n",
       "beta[14]  -0.14  1.9e-3   0.09  -0.33   -0.2  -0.14  -0.07   0.03   2393    1.0\n",
       "beta[15]  -0.28  2.2e-3   0.12  -0.52  -0.35  -0.28   -0.2  -0.08   2839    1.0\n",
       "beta[16]   0.27  1.6e-3   0.08   0.11   0.21   0.27   0.33   0.43   2786    1.0\n",
       "beta[17]  -0.32  1.9e-3   0.11  -0.53  -0.39  -0.32  -0.25  -0.12   3107    1.0\n",
       "beta[18]   0.33  2.5e-3   0.12    0.1   0.24   0.33   0.42   0.57   2546    1.0\n",
       "beta[19]   0.27  2.2e-3   0.11   0.04    0.2   0.27   0.35   0.49   2615    1.0\n",
       "beta[20]   0.13  3.0e-3   0.14  -0.12   0.04   0.13   0.22    0.4   1989    1.0\n",
       "beta[21]  -0.06  3.2e-3   0.15  -0.34  -0.15  -0.06   0.03   0.22   2040    1.0\n",
       "beta[22]  -0.09  1.8e-3   0.09  -0.28  -0.15  -0.09  -0.03   0.08   2675    1.0\n",
       "beta[23]  -0.01  2.7e-3   0.13  -0.27   -0.1  -0.02   0.07   0.24   2294    1.0\n",
       "beta[24]-4.6e-3  2.8e-3   0.13  -0.24  -0.09-8.3e-3   0.08   0.25   2149    1.0\n",
       "lp__     -482.9    0.12   3.62 -491.0 -485.1 -482.6 -480.2 -476.8    877    1.0\n",
       "\n",
       "Samples were drawn using NUTS at Thu Dec 20 19:44:04 2018.\n",
       "For each parameter, n_eff is a crude measure of effective sample size,\n",
       "and Rhat is the potential scale reduction factor on split chains (at \n",
       "convergence, Rhat=1)."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitBlrNuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical bayesian logistic regression\n",
    "Same beta and alpha coefficients as before but now also the variance parameter is given an exponential prior.\n",
    "\n",
    "Furthermore interaction effects are added to the model which results to 300-dimensional vector of predictors and 300-dimensional vector of coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 300) (1000, 24)\n",
      "[[-1.25456565 -1.23647786  1.38728734 -0.73343195  1.83316907  1.33807849\n",
      "   0.44932648  1.04698668 -1.29372298  2.76645648  0.46083068  1.02707891\n",
      "  -0.42828957  1.21459768 -0.19601428 -0.55270519 -0.33886163  0.32021217\n",
      "  -0.20676767 -0.4669334   0.63444822 -0.14998296 -0.5         0.76635604]\n",
      " [-0.45902624  2.24819436 -0.74263181  0.96637654 -0.69970702 -0.31795924\n",
      "  -0.96364986 -0.76597727 -1.29372298 -1.19140394  0.46083068 -0.704926\n",
      "  -0.42828957 -0.82331789 -0.19601428 -0.55270519 -0.33886163  0.32021217\n",
      "  -0.20676767 -0.4669334   0.63444822 -0.14998296 -0.5         0.76635604]]\n",
      "[[-1.25456565 -1.23647786  1.38728734 -0.73343195  1.83316907  1.33807849\n",
      "   0.44932648  1.04698668 -1.29372298  2.76645648  0.46083068  1.02707891\n",
      "  -0.42828957  1.21459768 -0.19601428 -0.55270519 -0.33886163  0.32021217\n",
      "  -0.20676767 -0.4669334   0.63444822 -0.14998296 -0.5         0.76635604\n",
      "   1.55124265 -1.74044304  0.92013854 -2.29983094 -1.67870731 -0.56370956]\n",
      " [-0.45902624  2.24819436 -0.74263181  0.96637654 -0.69970702 -0.31795924\n",
      "  -0.96364986 -0.76597727 -1.29372298 -1.19140394  0.46083068 -0.704926\n",
      "  -0.42828957 -0.82331789 -0.19601428 -0.55270519 -0.33886163  0.32021217\n",
      "  -0.20676767 -0.4669334   0.63444822 -0.14998296 -0.5         0.76635604\n",
      "  -1.0319802   0.34088749 -0.44359219  0.32118388  0.14595164  0.44234057]]\n"
     ]
    }
   ],
   "source": [
    "# Create new x-vector for the hierarchical model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "hCreditX=PolynomialFeatures(2, interaction_only=True, include_bias=False).fit_transform(creditX)\n",
    "print(hCreditX.shape, creditX.shape)\n",
    "print(creditX[:2,])\n",
    "print(hCreditX[:2,:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_3e737708637464be719ee19b1a2c8195 NOW.\n"
     ]
    }
   ],
   "source": [
    "# Bayesian logistic regression, hierarchical model\n",
    "blrhModel = \"\"\"\n",
    "    data {\n",
    "      int<lower=0> N;\n",
    "      int<lower=0> K;   // number of predictors      \n",
    "      matrix[N,K] x;\n",
    "      int<lower=0,upper=1> y[N];\n",
    "    }\n",
    "    parameters {\n",
    "      real alpha;\n",
    "      vector[K] beta;\n",
    "      real<lower=0> sigma2 ; \n",
    "    }\n",
    "    model {\n",
    "      sigma2 ~ exponential(rate = 0.01)\n",
    "      alpha~normal(0,sigma2);\n",
    "      beta~normal(0,sigma2);  \n",
    "      y ~ bernoulli_logit(alpha + x*beta);\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "credit_dat = {'x': hCreditX,\n",
    "                'y': creditY,\n",
    "                'N': len(creditY),\n",
    "                'K': hCreditX.shape[1]\n",
    "             }\n",
    "\n",
    "smBlrh = pystan.StanModel(model_code=blrModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitBlrhNuts = smBlrh.sampling(data=credit_dat, iter=1000, chains=4, algorithm='NUTS', control=dict(max_treedepth=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitBlrhNuts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250-dimensional MVN\n",
    "mvn250Model = \"\"\"\n",
    "    data {\n",
    "        int wishSize;\n",
    "        matrix[wishSize,wishSize] A;\n",
    "        vector[wishSize] mean0;\n",
    "    }\n",
    "    parameters {\n",
    "        vector[wishSize] theta;\n",
    "    }\n",
    "    model{\n",
    "        theta ~ multi_normal(mean0, A);\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "from scipy.stats import wishart\n",
    "wishSize=250\n",
    "precisionMatrixA=wishart.rvs(df=wishSize, scale=np.identity(wishSize), random_state=None)\n",
    "\n",
    "wishart_data={'A': precisionMatrixA, 'mean0': np.zeros(wishSize), 'wishSize': wishSize}\n",
    "\n",
    "sm = pystan.StanModel(model_code=mvn250Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitWishartNuts = sm.sampling(data=wishart_data, iter=1000, chains=4, algorithm='NUTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitWishartNuts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
